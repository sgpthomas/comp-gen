#!/usr/bin/env python3

import csv
import re
from abc import ABC, abstractmethod
from datetime import datetime
from functools import reduce
from pathlib import Path
from re import Match
from typing import Any, Callable, Dict, Iterator, List, Tuple

import click


def matches(regex, fun: Callable) -> Callable:
    """
    Returns a function that searches for `regex` in some input.
    If there are any matches, `fun` is called on that input.
    """

    def f(input):
        matches = re.search(regex, input)
        if matches is not None:
            return fun(matches)
        else:
            return None

    return f


def dict_to_csv(d: Dict[str, str]):
    """
    Returns an iterator of the CSV representation of a python dictionary
    """

    for name, value in d.items():
        if isinstance(value, dict):
            # add name to every row generated by the recursive
            # call, and then yield those rows individually
            prev_rows = dict_to_csv(value)
            new_rows = [[name] + row for row in prev_rows]
            for row in new_rows:
                yield row
        else:
            yield [name, value]


def time_to_float(f: str, ending: str) -> float:
    """
    Return the float representation of the number of seconds represents by {f}{µs|ms|s}
    """
    if ending == "µs":
        return float(f) * 1e-6
    elif ending == "ms":
        return float(f) * 0.001
    elif ending == "s":
        return float(f)
    else:
        raise Exception(ending)


class LogFilter(ABC):
    """
    Extract recursively structured data from a log by defining filter that
    represent the recursive structure of the data.

    `LogFilter` allows you to write very simple line-level filters, and compose them
    into something that extracts information into a structured format.

    Suppose that you had a log of this form:
    ```log
    Starting Phase 1
    property a: 10
    property b: 20
    property c: 30
    Starting Phase 2
    property a: 11
    property b: 21
    property c: 31
    Starting Phase 3
    property a: 12
    property b: 22
    property c: 32
    ```

    This log corresponds to some `Phase` data-structure that has 3 properties defined.

    To extract this information using `LogFilter`, you could use a `Chunker` that
    that splits the log into 3 sub-logs; 1 for each phase.

    Then you could define a filter that takes the lines of the form `property a: 10`,
    and produce a datum like: {'a': 10}.

    The code for parsing this sample log file would look something like this:
    ```python
    Chunker(
      start=matches(r"Starting Phase (\w+)", lambda: m: m.group(1)),
      data=LineFilter(r"property (\w+): (\d+)", lambda m: {m.group(1): int(m.group(2))})
    )
    ```

    Because `data` just takes another LogFilter, you can abitrarily nest them to
    extract structured data from logs.
    """

    def __init__(self, combine: Callable | None = None):
        if isinstance(combine, Callable):
            self.combine = combine
        else:
            self.combine = lambda x: x

    @abstractmethod
    def step(self, log) -> Iterator[Dict]:
        """
        TODO(sam): write docs
        """

        pass

    def run(self, log: Iterator[str]) -> Dict:
        """
        TODO(sam): write docs
        """

        res = []
        for datum in self.step(log):
            res.append(datum)

        return self.combine(res)


class IdentFilter(LogFilter):
    def __init__(self, combine: Callable | None = None):
        super().__init__(combine)

    def step(self, log) -> Iterator[Dict]:
        return log


class Chunker(LogFilter):
    """
    Chunks up a log into a series of "chunked" logs, and returns the result
    of another LogFilter applied to these chunks.

    A chunk is defined by a `start` filter, and optionally an `end` filter.
    A chunk starts the first time a line matches `start`. If an `end` filter
    is defined, a chunk ends when the `end` filter matches. If no `end` filter
    is defined, then a chunk ends the next time `start` matches
    (or the end of the log).

    By default, this filter produces a dictionary where the key of each entry
    is whatever the `start` filter returns on a matched line. The value is
    whatever is returned by the `data` LogFilter.

    An optional `combine` parameter can be given that specifies how to combine
    the list of returned dictionaries into a single object. If nothing is
    specified, a list of dictionaries is returned.
    """

    def __init__(
        self,
        start,
        end: Callable | None = None,
        data: LogFilter | None = None,
        combine=None,
    ):
        super().__init__(combine)
        self.start = start

        # if we have no end, then have it be the function that
        # always returns None
        self.end: Callable
        if end is None:
            self.end = lambda _: None
        else:
            self.end = end

        # the default log filter is the id filter
        if data is None:
            self.data = IdentFilter()
        else:
            self.data = data

    def step(self, log: Iterator[str]) -> Iterator[Dict]:
        chunk: Tuple[str, List[str]] | None = None
        for line in log:
            # the start filter matches
            if self.start(line) is not None:
                # if we have a chunk, then yield it
                if chunk is not None:
                    yield {chunk[0]: self.data.run(iter(chunk[1]))}

                # start a new chunk with this line as head
                chunk = (self.start(line), [])

            # if the end filter matches and we have a chunk
            # add the ending line to the chunk, yield it,
            # and then reset it
            elif self.end(line) is not None and chunk is not None:
                chunk[1].append(self.end(line))
                yield {chunk[0]: self.data.run(iter(chunk[1]))}
                chunk = None

            # if we are working on a chunk, add the current line
            # to that chunk
            elif chunk is not None:
                chunk[1].append(line)

        # if we reach the end of the log and still have a chunk
        # yield it
        if chunk is not None:
            yield {chunk[0]: self.data.run(iter(chunk[1]))}


class LineFilter(LogFilter):
    """
    Filter that applies `f` for any line in the log that matches `regex`.
    """

    def __init__(self, regex: str, f: Callable[[Match[str]], Any], combine=None):
        super().__init__(combine)
        self.regex = regex
        self.f = f

    def step(self, log):
        for line in log:
            matches: Match[str] | None = re.search(self.regex, line)
            if matches is not None:
                yield self.f(matches)


class First(LineFilter):
    """
    Line filter that yields the result of `f` for the first line that matches `regex`.
    """

    def __init__(self, regex: str, f: Callable[[Match[str]], Any]):
        super().__init__(regex, f, combine=lambda x: x[0] if len(x) > 0 else {})


class Combine(IdentFilter):
    """
    Composes a list of LogFilters together into a single LogFilter that applies
    every child filter on each chunk of the log.
    """

    def __init__(self, *children: LogFilter, combine=None):
        if combine is None:
            super().__init__(None)
        else:
            super().__init__(lambda x: combine(sum(x, [])))
        self.children = children

    def run(self, log):
        res = []
        for child in self.children:
            res.append(child.run(log))

        return self.combine(res)


class DictCombinator:
    @staticmethod
    def flatten(dicts: List[Dict[Any, Any]]) -> Dict[Any, Any]:
        """
        Merge a list of dicts into a single dictionary. Dictionaries
        later in the input list, override values present in earlier
        dictionaries if there are conflicting keys.
        """

        if len(dicts) == 0:
            return {}
        else:
            return reduce(lambda a, b: {**a, **b}, dicts)

    @staticmethod
    def uniquify(inp: List[Dict[Any, Any]]) -> Dict[Any, Any]:
        """
        Merge dictionary, renaming keys based on the order
        they appear in so that they are unique. This prevents
        all key conflicts.
        """
        res = {}
        for i, d in enumerate(inp):
            for k, v in d.items():
                res[f"{k}-{i}"] = v
        return res


def pp(f):
    def inner(x):
        print("")
        print("input:", x)
        return f(x)

    return inner


def filter_log(log):
    # matches logs of the form:
    # ```stderr.log
    #   Runner report
    #   =============
    #     Phase: 'pre-compile' with 30 rules
    #     Stop reason: IterationLimit(2)
    #     Iterations: 2
    #     Cost: 56582.11600000001 (old: 56582.11600000001)
    #     Time: 0.0017421899999999998
    #     Egraph size: 1048 nodes, 206 classes, 1079 memo
    # ======================
    # ```
    cost = Chunker(
        start=matches("Runner report", lambda _: "report"),
        end=matches("======================", lambda _: ""),
        combine=lambda x: x[0] if len(x) > 0 else {},
        data=Combine(
            First(r"Stop reason: (.*)", lambda m: {"stop_reason": m.group(1)}),
            First(
                r"Cost: (\d+.\d+) \(old: .*\)",
                lambda m: {"cost": float(m.group(1))},
            ),
            First(
                r"Time: (\d+.\d+)",
                lambda m: {"time": float(m.group(1))},
            ),
            First(r"Iterations: (\d+)", lambda m: {"iterations": int(m.group(1))}),
            First(
                r"Egraph size: (\d+) nodes, (\d+) classes",
                lambda m: {
                    "final_nodes": int(m.group(1)),
                    "final_classes": int(m.group(2)),
                },
            ),
            combine=DictCombinator.flatten,
        ),
    )

    # line filter that extracts number of nodes and classes from
    # [egg::run] log messages
    egraph_stats = LineFilter(
        r"Size: n=(\d+), e=(\d+)",
        lambda m: {"nodes": int(m.group(1)), "classes": int(m.group(2))},
    )

    # line filter that extracts search time from [egg::run] log messages
    search_time = LineFilter(
        r"Search time: (\d+\.\d+)", lambda m: {"search": float(m.group(1))}
    )

    # line filter that extracts apply time from [egg::run] log messages
    apply_time = LineFilter(
        r"Apply time: (\d+\.\d+)", lambda m: {"apply": float(m.group(1))}
    )

    # phase chunks. each chunk starts with the phrase "Starting Phase"
    phases_f = Chunker(
        start=matches(r"Starting Phase (\w+)", lambda m: m.group(1)),
        combine=DictCombinator.uniquify,
        data=Combine(
            Chunker(
                start=matches(r"Iteration (\d+)", lambda m: int(m.group(1))),
                combine=DictCombinator.flatten,
                data=Combine(
                    search_time,
                    egraph_stats,
                    apply_time,
                    LineFilter(
                        r"Best cost so far: (\d+\.\d+)",
                        lambda m: {"cost": float(m.group(1))},
                    ),
                    LineFilter(
                        r"\[(.*) INFO .*\] Best cost so far",
                        lambda m: {
                            "timestamp": datetime.timestamp(
                                datetime.strptime(m.group(1), "%Y-%m-%dT%H:%M:%SZ")
                            )
                        },
                    ),
                    LineFilter(
                        r"Extraction took: (\d+\.\d+)(.*)",
                        lambda m: {"extraction": time_to_float(m.group(1), m.group(2))},
                    ),
                    combine=DictCombinator.flatten,
                ),
            ),
            cost,
            First(r"Using (\d+) rules", lambda m: {"rules": int(m.group(1))}),
            First(
                r"Initial Program Depth: (\d+)",
                lambda m: {"initial_depth": int(m.group(1))},
            ),
            First(
                r"Final Program Depth: (\d+)",
                lambda m: {"final_depth": int(m.group(1))},
            ),
            combine=DictCombinator.flatten,
        ),
    )

    return phases_f.run(log)


def process_single(data_path):
    stderr_log = data_path / "stderr.log"
    log = stderr_log.open("r").readlines()
    log = map(lambda x: x.strip(), log)

    print(f"Reading {data_path}...", end="")
    data = filter_log(log)

    # pp = pprint.PrettyPrinter(indent=2)
    # print()
    # pp.pprint(data)

    # write out to a csv
    res = list(dict_to_csv(data))
    with (data_path / "data.csv").open("w") as f:
        wr = csv.writer(f)
        wr.writerow(["phase", "iteration", "name", "value"])
        wr.writerows(list(res))

    print("Done")


@click.group()
def cli():
    pass


@cli.command()
@click.argument("data_dir")
def single(data_dir):
    process_single(Path(data_dir))


@cli.command()
@click.argument("parent_dir")
@click.option("--force", is_flag=True)
def all(parent_dir, force):
    parent_dir = Path(parent_dir)

    # find all directories that have a stderr.log
    for log_path in parent_dir.glob("**/stderr.log"):
        if force or not (log_path.parents[0] / "data.csv").exists():
            # call process single on the containing directory
            # that's what `.parents[0]` gets us.
            process_single(log_path.parents[0])


if __name__ == "__main__":
    cli()
